---
title: "Machine Learning Project"
output: html_document
---

**Feature selection:**
The data file consists of observations on 6 participants performing a specific exercise program. Their performance was classified into classes A, B, C, D, E. Class A consists of exercise routines performed according to the instructions. Classes B, C, D,and, E  consisted of exercise routines performed with common mistakes. The original data file used for this project consists of 19622 rows and 160 columns. The observations are based on four sensors - in the lumbar belt, arm, forearm and dumbbells. Each sensor measured features on Euler angles - roll, pitch, and yaw. In addition, the mean, variance, std-dev, max, min, amplitude, kurtosis and skewness were reported for each Euler angle. Raw accelerator, gyroscope, and magnetometer readings along the x-, y-, and z- axes were also measured. 

On examining the data one finds that the observations on the mean, variance, std-dev, max, min, amplitude, kurtosis and skewness, for the roll, pitch and yaw measurements were mostly missing. I eliminated all these features from the feature set. I also eliminated the time window columns. The tidy data set had only 53 columns. 

Next I partitioned this tidy data set (70%-30% split) into a training set of dimensions 13737 X 53, and the testing set of dimensions 5885 X 53. 

**Choice of training method:** As this is a classification problem with 5 output classes, two methods that seemed to be good candidates for training were the random forests classification (rf) and the generalized boosting method (gbm). I tried both the methods.

**Training with random forest method:**
```
Random Forest 
13737 samples
   52 predictors
    5 classes: 'A', 'B', 'C', 'D', 'E' 
No pre-processing
Resampling: Bootstrapped (25 reps) 

Resampling results across tuning parameters:
  mtry  Accuracy  Kappa  Accuracy SD  Kappa SD
   2    0.989     0.986  0.00166      0.00209 
  27    0.988     0.985  0.00132      0.00166 
  52    0.980     0.975  0.00415      0.00527 

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 2. 
```
```
Confusion Matrix and Statistics on the testing set:

          Reference
Prediction    A    B    C    D    E
         A 1672    2    0    0    0
         B    7 1129    3    0    0
         C    0   10 1015    1    0
         D    0    0   13  948    3
         E    0    0    1    4 1077

Overall Statistics
                                      Accuracy : 0.9925        
                
Statistics by Class:

                      Class: A  Class: B Class: C Class: D  Class: E
Sensitivity            0.9958   0.9895   0.9835   0.9948    0.9972
Specificity            0.9995   0.9979   0.9977   0.9968    0.9990
Pos Pred Value         0.9988   0.9912   0.9893   0.9834    0.9954
Neg Pred Value         0.9983   0.9975   0.9965   0.9990    0.9994
Prevalence             0.2853   0.1939   0.1754   0.1619    0.1835
Detection Rate         0.2841   0.1918   0.1725   0.1611    0.1830
Detection Prevalence   0.2845   0.1935   0.1743   0.1638    0.1839
Balanced Accuracy      0.9977   0.9937   0.9906   0.9958    0.9981

```

**Training using GBM:**
```
Stochastic Gradient Boosting 

13737 samples
   52 predictors
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Bootstrapped (25 reps) 
Resampling results across tuning parameters:

  interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD
  1                   50      0.747     0.679  0.00773      0.00988 
  1                  100      0.816     0.768  0.00575      0.00735 
  1                  150      0.848     0.808  0.00446      0.00571 
  2                   50      0.852     0.812  0.00501      0.00644 
  2                  100      0.903     0.877  0.00366      0.00467 
  2                  150      0.928     0.909  0.00405      0.00514 
  3                   50      0.893     0.865  0.00442      0.00561 
  3                  100      0.938     0.922  0.00285      0.00361 
  3                  150      0.957     0.946  0.00353      0.00448 

Tuning parameter 'shrinkage' was held constant at a value of 0.1
Accuracy was used to select the optimal model using  the largest value.
The final values used for the model were n.trees = 150, interaction.depth =
 3 and shrinkage = 0.1. 
```
```
Confusion Matrix and Statistics on the test set

          Reference
Prediction    A    B    C    D    E
         A 1642   20    4    4    4
         B   45 1061   30    2    1
         C    0   38  976   10    2
         D    1    4   23  927    9
         E    1    7   10   11 1053

Overall Statistics
                                          
               Accuracy : 0.9616          
          
Statistics by Class:
                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9722   0.9389   0.9358   0.9717   0.9850
Specificity            0.9924   0.9836   0.9897   0.9925   0.9940
Pos Pred Value         0.9809   0.9315   0.9513   0.9616   0.9732
Neg Pred Value         0.9888   0.9855   0.9862   0.9945   0.9967
Prevalence             0.2870   0.1920   0.1772   0.1621   0.1816
Detection Rate         0.2790   0.1803   0.1658   0.1575   0.1789
Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
Balanced Accuracy      0.9823   0.9613   0.9627   0.9821   0.9895

```

**Conclusion:**
The overall accuracy for the random forest training method is 0.9925, and for the gbm method is 0.9616. Both the methods have worked well in predicting the values of the test set. The random forest methods is almost 99% accurate. The sensitivity and specificity for all classes is much higher in the random forests method (> 99%) than the gbm. Thus the random forests method in this case works much better than the "gbm" method. Both methods have the predicted the same outcomes on the 20 cases provided in the problem.
 
 The prediction on the set with 20 cases given by both methods is:

```
Case ID:    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Prediction: B A B A A E D B A A  B  C  B  A   E  E  A  B  B  B
```

**Future Direction for this project.**: The computer run time for both the methods was significant (more than an hour). Better selection of features (fewer than 52) may bring down this run time. 

